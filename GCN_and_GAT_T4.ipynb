{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Implimentation\n",
    "\n",
    "### ---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Convolution Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# import important libbraries\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#for plotting\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tqdm\n",
    "\n",
    "# pytorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "# torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a pytorch module\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "\n",
    "    def __init__(self,c_in,c_out):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(c_in,c_out)\n",
    "\n",
    "    def forward(self,node_feats,adj_matrix):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "\n",
    "            node_feats = Tensor with node features of shape[batch_size, num_nodes, c_in]\n",
    "            adj_matrix = Batch adjecency matrix of the graph\n",
    "            shape:[batch_size, num_nodes,num_nodes]\n",
    "                    \n",
    "        \"\"\"\n",
    "        #num_neighbors = number of incoming edges\n",
    "        num_neignbors = adj_matrix.sum(dim = -1,keepdims=True)\n",
    "        node_feats = self.projection(node_feats)\n",
    "        node_feats = torch.bmm(adj_matrix,node_feats)\n",
    "        node_feats = node_feats / num_neignbors\n",
    "        return node_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's apply it to example graph\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "# creating graph from external resources\n",
    "\n",
    "nx_g = nx.from_edgelist([(2,1),(2,3),(4,2),(3,4)])\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#nx.draw(g, with_labels=True)\n",
    "#plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "nx.draw(nx_g, with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node featurs:\n",
      " tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "\n",
      "Adjecency matrix:\n",
      " tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "# defining node features\n",
    "\n",
    "node_feats = torch.arange(8, dtype=torch.float32).view(1,4,2)\n",
    "\n",
    "adj_matrix = torch.Tensor([[[1, 1, 0, 0],\n",
    "                            [1, 1, 1, 1],\n",
    "                            [0, 1, 1, 1],\n",
    "                            [0, 1, 1, 1]]])\n",
    "\n",
    "print(\"Node featurs:\\n\",node_feats)\n",
    "print(\"\\nAdjecency matrix:\\n\", adj_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjecency matrix tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]]])\n",
      "Input feature tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "Output features tensor([[[1., 2.],\n",
      "         [3., 4.],\n",
      "         [4., 5.],\n",
      "         [4., 5.]]])\n"
     ]
    }
   ],
   "source": [
    "# Now let's apply a GCN layer to it: for simplycity we initialize linear weight matrix as identity mtrix so that input features are equal to the message\n",
    "\n",
    "layer = GCNLayer(c_in=2,c_out=2)\n",
    "\n",
    "layer.projection.weight.data = torch.Tensor([[1.,0.],[0.,1.]])\n",
    "layer.projection.bias.data = torch.Tensor([0.,0.])\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_feats = layer(node_feats,adj_matrix)\n",
    "\n",
    "print(\"Adjecency matrix\", adj_matrix)\n",
    "print(\"Input feature\",node_feats)\n",
    "print(\"Output features\",out_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Attention Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "\n",
    "    def __init__(self,c_in,c_out,num_heads=1,concat_heads=True,alpha=0.2):\n",
    "\n",
    "        \"\"\"Inputs:\n",
    "\n",
    "                c_in = dimentionality of input features\n",
    "                c_out = dimentionality of output features\n",
    "\n",
    "                num_heads = number of heads in attention mechanism\n",
    "                conacat = if True outputs of all heads will be concatinated instead of averaging\n",
    "                alpha= negative slope of leaky relu      \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.concat_heads = concat_heads\n",
    "\n",
    "        if self.concat_heads:\n",
    "            assert c_out % num_heads == 0 ; \"Number of output features must be the multiple of the count of heads\"\n",
    "            c_out = c_out // num_heads\n",
    "\n",
    "        # sub module and parameters needed in the layer\n",
    "\n",
    "        self.projection = nn.Linear(c_in,c_out*num_heads)\n",
    "        self.a = nn.Parameter(torch.Tensor(num_heads, 2 * c_out)) # one per head\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "\n",
    "        # Initialization from the original implementation\n",
    "        nn.init.xavier_uniform_(self.projection.weight.data, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "    def forward(self,node_feats,adj_matrix,print_attn_probs=False):\n",
    "\n",
    "        batch_size,num_nodes = node_feats.size(0),node_feats.size(1)\n",
    "\n",
    "        # apply linear layer\n",
    "        node_feats = self.projection(node_feats)\n",
    "        node_feats = node_feats.view(batch_size,num_nodes,self.num_heads,-1)\n",
    "\n",
    "        # we need to calculate the attention logits for every edge in the adjecency matrix\n",
    "        # doing this on all possible nodes will be very expensive\n",
    "        \n",
    "        edges = adj_matrix.nonzero(as_tuple=False) # return indices where adjecency matrix is not zero\n",
    "\n",
    "        node_feats_flat = node_feats.view(batch_size*num_nodes, self.num_heads,-1)\n",
    "\n",
    "        edge_indices_row = edges[:,0] * num_nodes + edges[:,1]\n",
    "        edge_indices_col = edges[:,0] * num_nodes + edges[:,2]\n",
    "        a_input = torch.cat([\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_row, dim=0),\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_col, dim=0)\n",
    "        ], dim=-1) # Index select returns a tensor with node_feats_flat being indexed at the desired positions along dim=0\n",
    "\n",
    "        # Calculate attention MLP output (independent for each head)\n",
    "        attn_logits = torch.einsum('bhc,hc->bh', a_input, self.a)\n",
    "        attn_logits = self.leakyrelu(attn_logits)\n",
    "\n",
    "        # Map list of attention values back into a matrix\n",
    "        attn_matrix = attn_logits.new_zeros(adj_matrix.shape+(self.num_heads,)).fill_(-9e15)\n",
    "        attn_matrix[adj_matrix[...,None].repeat(1,1,1,self.num_heads) == 1] = attn_logits.reshape(-1)\n",
    "\n",
    "        # Weighted average of attention\n",
    "        attn_probs = F.softmax(attn_matrix, dim=2)\n",
    "        if print_attn_probs:\n",
    "            print(\"Attention probs\\n\", attn_probs.permute(0, 3, 1, 2))\n",
    "        node_feats = torch.einsum('bijh,bjhc->bihc', attn_probs, node_feats)\n",
    "\n",
    "        # If heads should be concatenated, we can do this by reshaping. Otherwise, take mean\n",
    "        if self.concat_heads:\n",
    "            node_feats = node_feats.reshape(batch_size, num_nodes, -1)\n",
    "        else:\n",
    "            node_feats = node_feats.mean(dim=2)\n",
    "\n",
    "        return node_feats\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention probs\n",
      " tensor([[[[0.3543, 0.6457, 0.0000, 0.0000],\n",
      "          [0.1096, 0.1450, 0.2642, 0.4813],\n",
      "          [0.0000, 0.1858, 0.2885, 0.5257],\n",
      "          [0.0000, 0.2391, 0.2696, 0.4913]],\n",
      "\n",
      "         [[0.5100, 0.4900, 0.0000, 0.0000],\n",
      "          [0.2975, 0.2436, 0.2340, 0.2249],\n",
      "          [0.0000, 0.3838, 0.3142, 0.3019],\n",
      "          [0.0000, 0.4018, 0.3289, 0.2693]]]])\n",
      "Adjacency matrix tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]]])\n",
      "Input features tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "Output features tensor([[[1.2913, 1.9800],\n",
      "         [4.2344, 3.7725],\n",
      "         [4.6798, 4.8362],\n",
      "         [4.5043, 4.7351]]])\n"
     ]
    }
   ],
   "source": [
    "layer = GATLayer(2,2, num_heads=2)\n",
    "layer.projection.weight.data = torch.Tensor([[1.,0.],[0.,1.]])\n",
    "layer.projection.bias.data = torch.Tensor([0.,0.])\n",
    "\n",
    "layer.a.data=  torch.Tensor([[-0.2,0.3],[0.1,-0.1]])\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_feats = layer(node_feats,adj_matrix,print_attn_probs=True)\n",
    "\n",
    "print(\"Adjacency matrix\", adj_matrix)\n",
    "print(\"Input features\", node_feats)\n",
    "print(\"Output features\", out_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pytorch Geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "import torch_geometric.nn as g_nn\n",
    "import torch_geometric.data as g_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment on Graph Structures\n",
    "\n",
    "##### We Fcous on Node Level task : Semi-Supervised Node classification\n",
    "\n",
    "A popular example that we will use in this tutorial is the Cora dataset, a citation network among papers. The Cora consists of 2708 scientific publications with links between each other representing the citation of one paper by another. The task is to classify each publication into one of seven classes. Each publication is represented by a bag-of-words vector. This means that we have a vector of 1433 elements for each publication, where a 1 at feature  indicates that the -th word of a pre-defined dictionary is in the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "dataset = torch_geometric.datasets.Planetoid(root=\"PyTorch_Practice_Models\" ,name=\"Cora\", transform=NormalizeFeatures())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataset[0]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(1433, 16)\n",
      "  (conv2): GCNConv(16, 7)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# GCN using pytorch-geometric librarry\n",
    "\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12)\n",
    "        self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.9466\n",
      "Epoch: 002, Loss: 1.9405\n",
      "Epoch: 003, Loss: 1.9344\n",
      "Epoch: 004, Loss: 1.9276\n",
      "Epoch: 005, Loss: 1.9177\n",
      "Epoch: 006, Loss: 1.9085\n",
      "Epoch: 007, Loss: 1.8978\n",
      "Epoch: 008, Loss: 1.8913\n",
      "Epoch: 009, Loss: 1.8760\n",
      "Epoch: 010, Loss: 1.8672\n",
      "Epoch: 011, Loss: 1.8529\n",
      "Epoch: 012, Loss: 1.8471\n",
      "Epoch: 013, Loss: 1.8397\n",
      "Epoch: 014, Loss: 1.8175\n",
      "Epoch: 015, Loss: 1.8009\n",
      "Epoch: 016, Loss: 1.7831\n",
      "Epoch: 017, Loss: 1.7812\n",
      "Epoch: 018, Loss: 1.7553\n",
      "Epoch: 019, Loss: 1.7327\n",
      "Epoch: 020, Loss: 1.7210\n",
      "Epoch: 021, Loss: 1.7216\n",
      "Epoch: 022, Loss: 1.6956\n",
      "Epoch: 023, Loss: 1.6848\n",
      "Epoch: 024, Loss: 1.6620\n",
      "Epoch: 025, Loss: 1.6428\n",
      "Epoch: 026, Loss: 1.6428\n",
      "Epoch: 027, Loss: 1.6133\n",
      "Epoch: 028, Loss: 1.5913\n",
      "Epoch: 029, Loss: 1.5533\n",
      "Epoch: 030, Loss: 1.5325\n",
      "Epoch: 031, Loss: 1.5367\n",
      "Epoch: 032, Loss: 1.5215\n",
      "Epoch: 033, Loss: 1.4778\n",
      "Epoch: 034, Loss: 1.4963\n",
      "Epoch: 035, Loss: 1.4301\n",
      "Epoch: 036, Loss: 1.4143\n",
      "Epoch: 037, Loss: 1.3961\n",
      "Epoch: 038, Loss: 1.3679\n",
      "Epoch: 039, Loss: 1.3656\n",
      "Epoch: 040, Loss: 1.3615\n",
      "Epoch: 041, Loss: 1.3219\n",
      "Epoch: 042, Loss: 1.2927\n",
      "Epoch: 043, Loss: 1.2782\n",
      "Epoch: 044, Loss: 1.2500\n",
      "Epoch: 045, Loss: 1.2356\n",
      "Epoch: 046, Loss: 1.2229\n",
      "Epoch: 047, Loss: 1.1588\n",
      "Epoch: 048, Loss: 1.2014\n",
      "Epoch: 049, Loss: 1.1507\n",
      "Epoch: 050, Loss: 1.1621\n",
      "Epoch: 051, Loss: 1.1263\n",
      "Epoch: 052, Loss: 1.1055\n",
      "Epoch: 053, Loss: 1.0458\n",
      "Epoch: 054, Loss: 1.0544\n",
      "Epoch: 055, Loss: 1.0135\n",
      "Epoch: 056, Loss: 1.0302\n",
      "Epoch: 057, Loss: 0.9965\n",
      "Epoch: 058, Loss: 0.9776\n",
      "Epoch: 059, Loss: 0.9440\n",
      "Epoch: 060, Loss: 0.9416\n",
      "Epoch: 061, Loss: 0.9490\n",
      "Epoch: 062, Loss: 0.9231\n",
      "Epoch: 063, Loss: 0.9221\n",
      "Epoch: 064, Loss: 0.8907\n",
      "Epoch: 065, Loss: 0.8969\n",
      "Epoch: 066, Loss: 0.8246\n",
      "Epoch: 067, Loss: 0.8113\n",
      "Epoch: 068, Loss: 0.8190\n",
      "Epoch: 069, Loss: 0.8218\n",
      "Epoch: 070, Loss: 0.8000\n",
      "Epoch: 071, Loss: 0.8205\n",
      "Epoch: 072, Loss: 0.8219\n",
      "Epoch: 073, Loss: 0.7665\n",
      "Epoch: 074, Loss: 0.7347\n",
      "Epoch: 075, Loss: 0.7833\n",
      "Epoch: 076, Loss: 0.7481\n",
      "Epoch: 077, Loss: 0.7337\n",
      "Epoch: 078, Loss: 0.7320\n",
      "Epoch: 079, Loss: 0.7360\n",
      "Epoch: 080, Loss: 0.6859\n",
      "Epoch: 081, Loss: 0.6695\n",
      "Epoch: 082, Loss: 0.6629\n",
      "Epoch: 083, Loss: 0.6756\n",
      "Epoch: 084, Loss: 0.6477\n",
      "Epoch: 085, Loss: 0.6175\n",
      "Epoch: 086, Loss: 0.6329\n",
      "Epoch: 087, Loss: 0.6550\n",
      "Epoch: 088, Loss: 0.6243\n",
      "Epoch: 089, Loss: 0.6541\n",
      "Epoch: 090, Loss: 0.6076\n",
      "Epoch: 091, Loss: 0.5844\n",
      "Epoch: 092, Loss: 0.5491\n",
      "Epoch: 093, Loss: 0.6345\n",
      "Epoch: 094, Loss: 0.5621\n",
      "Epoch: 095, Loss: 0.6057\n",
      "Epoch: 096, Loss: 0.5747\n",
      "Epoch: 097, Loss: 0.5956\n",
      "Epoch: 098, Loss: 0.5630\n",
      "Epoch: 099, Loss: 0.5894\n",
      "Epoch: 100, Loss: 0.5730\n"
     ]
    }
   ],
   "source": [
    "model = GCN(hidden_channels=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test():\n",
    "      model.eval()\n",
    "      out = model(data.x, data.edge_index)\n",
    "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
    "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "      return test_acc\n",
    "\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7910\n"
     ]
    }
   ],
   "source": [
    "test_acc = test()\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT(\n",
      "  (conv1): GATConv(1433, 8, heads=1)\n",
      "  (conv2): GATConv(8, 7, heads=1)\n",
      ")\n",
      "Epoch: 001, Loss: 1.9457, Val: 0.2000, Test: 0.2000\n",
      "Epoch: 002, Loss: 1.9421, Val: 0.2080, Test: 0.1740\n",
      "Epoch: 003, Loss: 1.9403, Val: 0.2240, Test: 0.2030\n",
      "Epoch: 004, Loss: 1.9368, Val: 0.2920, Test: 0.2750\n",
      "Epoch: 005, Loss: 1.9341, Val: 0.3940, Test: 0.3900\n",
      "Epoch: 006, Loss: 1.9304, Val: 0.4680, Test: 0.4560\n",
      "Epoch: 007, Loss: 1.9281, Val: 0.5620, Test: 0.5630\n",
      "Epoch: 008, Loss: 1.9228, Val: 0.5920, Test: 0.5820\n",
      "Epoch: 009, Loss: 1.9207, Val: 0.5560, Test: 0.5550\n",
      "Epoch: 010, Loss: 1.9206, Val: 0.5600, Test: 0.5660\n",
      "Epoch: 011, Loss: 1.9107, Val: 0.5680, Test: 0.5790\n",
      "Epoch: 012, Loss: 1.9135, Val: 0.6040, Test: 0.6000\n",
      "Epoch: 013, Loss: 1.9062, Val: 0.6600, Test: 0.6470\n",
      "Epoch: 014, Loss: 1.9065, Val: 0.7060, Test: 0.7050\n",
      "Epoch: 015, Loss: 1.8982, Val: 0.7340, Test: 0.7290\n",
      "Epoch: 016, Loss: 1.8996, Val: 0.7480, Test: 0.7310\n",
      "Epoch: 017, Loss: 1.8939, Val: 0.7440, Test: 0.7270\n",
      "Epoch: 018, Loss: 1.8918, Val: 0.7340, Test: 0.7180\n",
      "Epoch: 019, Loss: 1.8898, Val: 0.6980, Test: 0.6940\n",
      "Epoch: 020, Loss: 1.8868, Val: 0.6920, Test: 0.6830\n",
      "Epoch: 021, Loss: 1.8756, Val: 0.6820, Test: 0.6810\n",
      "Epoch: 022, Loss: 1.8789, Val: 0.6940, Test: 0.6810\n",
      "Epoch: 023, Loss: 1.8678, Val: 0.7060, Test: 0.6840\n",
      "Epoch: 024, Loss: 1.8643, Val: 0.6920, Test: 0.6810\n",
      "Epoch: 025, Loss: 1.8601, Val: 0.7080, Test: 0.6860\n",
      "Epoch: 026, Loss: 1.8611, Val: 0.7000, Test: 0.6850\n",
      "Epoch: 027, Loss: 1.8512, Val: 0.7120, Test: 0.6910\n",
      "Epoch: 028, Loss: 1.8490, Val: 0.7220, Test: 0.7070\n",
      "Epoch: 029, Loss: 1.8429, Val: 0.7340, Test: 0.7120\n",
      "Epoch: 030, Loss: 1.8442, Val: 0.7420, Test: 0.7140\n",
      "Epoch: 031, Loss: 1.8343, Val: 0.7420, Test: 0.7210\n",
      "Epoch: 032, Loss: 1.8370, Val: 0.7460, Test: 0.7240\n",
      "Epoch: 033, Loss: 1.8340, Val: 0.7460, Test: 0.7330\n",
      "Epoch: 034, Loss: 1.8124, Val: 0.7460, Test: 0.7370\n",
      "Epoch: 035, Loss: 1.8073, Val: 0.7460, Test: 0.7330\n",
      "Epoch: 036, Loss: 1.8094, Val: 0.7460, Test: 0.7360\n",
      "Epoch: 037, Loss: 1.8137, Val: 0.7440, Test: 0.7380\n",
      "Epoch: 038, Loss: 1.8009, Val: 0.7420, Test: 0.7320\n",
      "Epoch: 039, Loss: 1.7959, Val: 0.7280, Test: 0.7260\n",
      "Epoch: 040, Loss: 1.7925, Val: 0.7140, Test: 0.7130\n",
      "Epoch: 041, Loss: 1.7819, Val: 0.7080, Test: 0.7110\n",
      "Epoch: 042, Loss: 1.7912, Val: 0.6980, Test: 0.7070\n",
      "Epoch: 043, Loss: 1.7891, Val: 0.7100, Test: 0.7090\n",
      "Epoch: 044, Loss: 1.7599, Val: 0.7140, Test: 0.7140\n",
      "Epoch: 045, Loss: 1.7586, Val: 0.7220, Test: 0.7150\n",
      "Epoch: 046, Loss: 1.7469, Val: 0.7260, Test: 0.7170\n",
      "Epoch: 047, Loss: 1.7525, Val: 0.7260, Test: 0.7200\n",
      "Epoch: 048, Loss: 1.7368, Val: 0.7380, Test: 0.7260\n",
      "Epoch: 049, Loss: 1.7327, Val: 0.7500, Test: 0.7340\n",
      "Epoch: 050, Loss: 1.7251, Val: 0.7580, Test: 0.7480\n",
      "Epoch: 051, Loss: 1.7139, Val: 0.7660, Test: 0.7580\n",
      "Epoch: 052, Loss: 1.7155, Val: 0.7700, Test: 0.7590\n",
      "Epoch: 053, Loss: 1.7152, Val: 0.7760, Test: 0.7610\n",
      "Epoch: 054, Loss: 1.6856, Val: 0.7760, Test: 0.7610\n",
      "Epoch: 055, Loss: 1.7053, Val: 0.7760, Test: 0.7600\n",
      "Epoch: 056, Loss: 1.6720, Val: 0.7840, Test: 0.7650\n",
      "Epoch: 057, Loss: 1.6638, Val: 0.7880, Test: 0.7670\n",
      "Epoch: 058, Loss: 1.6527, Val: 0.7880, Test: 0.7650\n",
      "Epoch: 059, Loss: 1.6640, Val: 0.7840, Test: 0.7680\n",
      "Epoch: 060, Loss: 1.6338, Val: 0.7800, Test: 0.7720\n",
      "Epoch: 061, Loss: 1.6177, Val: 0.7840, Test: 0.7760\n",
      "Epoch: 062, Loss: 1.6234, Val: 0.7840, Test: 0.7780\n",
      "Epoch: 063, Loss: 1.6224, Val: 0.7760, Test: 0.7760\n",
      "Epoch: 064, Loss: 1.5954, Val: 0.7800, Test: 0.7770\n",
      "Epoch: 065, Loss: 1.6121, Val: 0.7860, Test: 0.7760\n",
      "Epoch: 066, Loss: 1.5814, Val: 0.7840, Test: 0.7760\n",
      "Epoch: 067, Loss: 1.5828, Val: 0.7800, Test: 0.7780\n",
      "Epoch: 068, Loss: 1.5615, Val: 0.7720, Test: 0.7770\n",
      "Epoch: 069, Loss: 1.5541, Val: 0.7680, Test: 0.7770\n",
      "Epoch: 070, Loss: 1.5304, Val: 0.7720, Test: 0.7810\n",
      "Epoch: 071, Loss: 1.5157, Val: 0.7760, Test: 0.7860\n",
      "Epoch: 072, Loss: 1.5175, Val: 0.7800, Test: 0.7920\n",
      "Epoch: 073, Loss: 1.5154, Val: 0.7820, Test: 0.7920\n",
      "Epoch: 074, Loss: 1.5001, Val: 0.7800, Test: 0.7950\n",
      "Epoch: 075, Loss: 1.4588, Val: 0.7780, Test: 0.7950\n",
      "Epoch: 076, Loss: 1.4533, Val: 0.7700, Test: 0.7900\n",
      "Epoch: 077, Loss: 1.4624, Val: 0.7680, Test: 0.7870\n",
      "Epoch: 078, Loss: 1.4363, Val: 0.7720, Test: 0.7840\n",
      "Epoch: 079, Loss: 1.4160, Val: 0.7660, Test: 0.7840\n",
      "Epoch: 080, Loss: 1.4605, Val: 0.7600, Test: 0.7820\n",
      "Epoch: 081, Loss: 1.3816, Val: 0.7560, Test: 0.7770\n",
      "Epoch: 082, Loss: 1.3659, Val: 0.7540, Test: 0.7710\n",
      "Epoch: 083, Loss: 1.3701, Val: 0.7540, Test: 0.7670\n",
      "Epoch: 084, Loss: 1.4274, Val: 0.7540, Test: 0.7650\n",
      "Epoch: 085, Loss: 1.4076, Val: 0.7500, Test: 0.7530\n",
      "Epoch: 086, Loss: 1.3621, Val: 0.7420, Test: 0.7480\n",
      "Epoch: 087, Loss: 1.3307, Val: 0.7360, Test: 0.7380\n",
      "Epoch: 088, Loss: 1.3810, Val: 0.7400, Test: 0.7240\n",
      "Epoch: 089, Loss: 1.3693, Val: 0.7200, Test: 0.7070\n",
      "Epoch: 090, Loss: 1.2485, Val: 0.7000, Test: 0.6900\n",
      "Epoch: 091, Loss: 1.3449, Val: 0.6860, Test: 0.6780\n",
      "Epoch: 092, Loss: 1.3193, Val: 0.6780, Test: 0.6680\n",
      "Epoch: 093, Loss: 1.2668, Val: 0.6540, Test: 0.6490\n",
      "Epoch: 094, Loss: 1.1893, Val: 0.6420, Test: 0.6370\n",
      "Epoch: 095, Loss: 1.2620, Val: 0.6320, Test: 0.6220\n",
      "Epoch: 096, Loss: 1.2062, Val: 0.6140, Test: 0.6130\n",
      "Epoch: 097, Loss: 1.2361, Val: 0.6080, Test: 0.6020\n",
      "Epoch: 098, Loss: 1.2222, Val: 0.6000, Test: 0.5970\n",
      "Epoch: 099, Loss: 1.1403, Val: 0.6000, Test: 0.5960\n",
      "Epoch: 100, Loss: 1.2017, Val: 0.5940, Test: 0.5930\n",
      "Epoch: 101, Loss: 1.1983, Val: 0.5940, Test: 0.5920\n",
      "Epoch: 102, Loss: 1.1827, Val: 0.5940, Test: 0.5930\n",
      "Epoch: 103, Loss: 1.1788, Val: 0.5940, Test: 0.5930\n",
      "Epoch: 104, Loss: 1.1728, Val: 0.6000, Test: 0.5970\n",
      "Epoch: 105, Loss: 1.1668, Val: 0.6040, Test: 0.6030\n",
      "Epoch: 106, Loss: 1.1975, Val: 0.6020, Test: 0.6030\n",
      "Epoch: 107, Loss: 1.1450, Val: 0.6080, Test: 0.6060\n",
      "Epoch: 108, Loss: 1.0360, Val: 0.6080, Test: 0.6070\n",
      "Epoch: 109, Loss: 1.1244, Val: 0.6060, Test: 0.5990\n",
      "Epoch: 110, Loss: 1.1889, Val: 0.6060, Test: 0.5950\n",
      "Epoch: 111, Loss: 1.0942, Val: 0.6040, Test: 0.6000\n",
      "Epoch: 112, Loss: 1.1259, Val: 0.5960, Test: 0.5960\n",
      "Epoch: 113, Loss: 1.0814, Val: 0.5980, Test: 0.5920\n",
      "Epoch: 114, Loss: 1.0741, Val: 0.5960, Test: 0.5900\n",
      "Epoch: 115, Loss: 1.0466, Val: 0.5940, Test: 0.5840\n",
      "Epoch: 116, Loss: 1.0673, Val: 0.5840, Test: 0.5790\n",
      "Epoch: 117, Loss: 1.0402, Val: 0.5840, Test: 0.5700\n",
      "Epoch: 118, Loss: 1.0430, Val: 0.5840, Test: 0.5750\n",
      "Epoch: 119, Loss: 1.0582, Val: 0.5860, Test: 0.5740\n",
      "Epoch: 120, Loss: 0.9822, Val: 0.5860, Test: 0.5770\n",
      "Epoch: 121, Loss: 1.0135, Val: 0.5860, Test: 0.5810\n",
      "Epoch: 122, Loss: 0.9855, Val: 0.5920, Test: 0.5890\n",
      "Epoch: 123, Loss: 0.9984, Val: 0.5980, Test: 0.5910\n",
      "Epoch: 124, Loss: 0.9497, Val: 0.6000, Test: 0.6020\n",
      "Epoch: 125, Loss: 1.0013, Val: 0.6060, Test: 0.6110\n",
      "Epoch: 126, Loss: 1.0230, Val: 0.6080, Test: 0.6220\n",
      "Epoch: 127, Loss: 0.9718, Val: 0.6200, Test: 0.6330\n",
      "Epoch: 128, Loss: 1.0050, Val: 0.6280, Test: 0.6360\n",
      "Epoch: 129, Loss: 1.0806, Val: 0.6320, Test: 0.6470\n",
      "Epoch: 130, Loss: 0.9892, Val: 0.6400, Test: 0.6540\n",
      "Epoch: 131, Loss: 0.9190, Val: 0.6380, Test: 0.6510\n",
      "Epoch: 132, Loss: 0.9925, Val: 0.6360, Test: 0.6460\n",
      "Epoch: 133, Loss: 0.9262, Val: 0.6340, Test: 0.6440\n",
      "Epoch: 134, Loss: 0.8980, Val: 0.6280, Test: 0.6400\n",
      "Epoch: 135, Loss: 0.8617, Val: 0.6220, Test: 0.6370\n",
      "Epoch: 136, Loss: 0.9173, Val: 0.6160, Test: 0.6310\n",
      "Epoch: 137, Loss: 0.9383, Val: 0.6080, Test: 0.6260\n",
      "Epoch: 138, Loss: 1.0035, Val: 0.6020, Test: 0.6210\n",
      "Epoch: 139, Loss: 0.8850, Val: 0.5960, Test: 0.6200\n",
      "Epoch: 140, Loss: 0.9282, Val: 0.6000, Test: 0.6120\n",
      "Epoch: 141, Loss: 0.9288, Val: 0.6000, Test: 0.6110\n",
      "Epoch: 142, Loss: 0.9073, Val: 0.5960, Test: 0.6080\n",
      "Epoch: 143, Loss: 0.8274, Val: 0.6000, Test: 0.6120\n",
      "Epoch: 144, Loss: 0.9621, Val: 0.6060, Test: 0.6170\n",
      "Epoch: 145, Loss: 0.9234, Val: 0.6100, Test: 0.6200\n",
      "Epoch: 146, Loss: 0.8663, Val: 0.6100, Test: 0.6240\n",
      "Epoch: 147, Loss: 0.8731, Val: 0.6120, Test: 0.6300\n",
      "Epoch: 148, Loss: 0.8892, Val: 0.6160, Test: 0.6310\n",
      "Epoch: 149, Loss: 0.8313, Val: 0.6140, Test: 0.6300\n",
      "Epoch: 150, Loss: 0.8999, Val: 0.6200, Test: 0.6340\n",
      "Epoch: 151, Loss: 0.7761, Val: 0.6260, Test: 0.6350\n",
      "Epoch: 152, Loss: 0.8447, Val: 0.6340, Test: 0.6410\n",
      "Epoch: 153, Loss: 0.8231, Val: 0.6340, Test: 0.6500\n",
      "Epoch: 154, Loss: 0.8678, Val: 0.6360, Test: 0.6490\n",
      "Epoch: 155, Loss: 0.8315, Val: 0.6360, Test: 0.6550\n",
      "Epoch: 156, Loss: 0.8431, Val: 0.6360, Test: 0.6630\n",
      "Epoch: 157, Loss: 0.8390, Val: 0.6400, Test: 0.6710\n",
      "Epoch: 158, Loss: 0.8405, Val: 0.6460, Test: 0.6740\n",
      "Epoch: 159, Loss: 0.8547, Val: 0.6480, Test: 0.6750\n",
      "Epoch: 160, Loss: 0.8274, Val: 0.6480, Test: 0.6740\n",
      "Epoch: 161, Loss: 0.8056, Val: 0.6500, Test: 0.6690\n",
      "Epoch: 162, Loss: 0.8449, Val: 0.6500, Test: 0.6700\n",
      "Epoch: 163, Loss: 0.7728, Val: 0.6560, Test: 0.6720\n",
      "Epoch: 164, Loss: 0.8419, Val: 0.6540, Test: 0.6730\n",
      "Epoch: 165, Loss: 0.7908, Val: 0.6520, Test: 0.6730\n",
      "Epoch: 166, Loss: 0.8459, Val: 0.6520, Test: 0.6720\n",
      "Epoch: 167, Loss: 0.7783, Val: 0.6500, Test: 0.6670\n",
      "Epoch: 168, Loss: 0.9015, Val: 0.6520, Test: 0.6700\n",
      "Epoch: 169, Loss: 0.8270, Val: 0.6540, Test: 0.6680\n",
      "Epoch: 170, Loss: 0.7137, Val: 0.6580, Test: 0.6720\n",
      "Epoch: 171, Loss: 0.8037, Val: 0.6620, Test: 0.6680\n",
      "Epoch: 172, Loss: 0.7570, Val: 0.6600, Test: 0.6660\n",
      "Epoch: 173, Loss: 0.7679, Val: 0.6560, Test: 0.6640\n",
      "Epoch: 174, Loss: 0.8398, Val: 0.6560, Test: 0.6630\n",
      "Epoch: 175, Loss: 0.7763, Val: 0.6560, Test: 0.6630\n",
      "Epoch: 176, Loss: 0.7739, Val: 0.6500, Test: 0.6620\n",
      "Epoch: 177, Loss: 0.8088, Val: 0.6520, Test: 0.6610\n",
      "Epoch: 178, Loss: 0.7546, Val: 0.6560, Test: 0.6650\n",
      "Epoch: 179, Loss: 0.7633, Val: 0.6540, Test: 0.6670\n",
      "Epoch: 180, Loss: 0.7414, Val: 0.6520, Test: 0.6710\n",
      "Epoch: 181, Loss: 0.8068, Val: 0.6520, Test: 0.6750\n",
      "Epoch: 182, Loss: 0.7497, Val: 0.6520, Test: 0.6780\n",
      "Epoch: 183, Loss: 0.7761, Val: 0.6540, Test: 0.6860\n",
      "Epoch: 184, Loss: 0.8337, Val: 0.6640, Test: 0.6900\n",
      "Epoch: 185, Loss: 0.7391, Val: 0.6640, Test: 0.6930\n",
      "Epoch: 186, Loss: 0.8486, Val: 0.6700, Test: 0.6920\n",
      "Epoch: 187, Loss: 0.6934, Val: 0.6720, Test: 0.6920\n",
      "Epoch: 188, Loss: 0.8068, Val: 0.6740, Test: 0.6960\n",
      "Epoch: 189, Loss: 0.7290, Val: 0.6760, Test: 0.6950\n",
      "Epoch: 190, Loss: 0.8155, Val: 0.6760, Test: 0.6950\n",
      "Epoch: 191, Loss: 0.7342, Val: 0.6800, Test: 0.7000\n",
      "Epoch: 192, Loss: 0.8145, Val: 0.6900, Test: 0.7050\n",
      "Epoch: 193, Loss: 0.6625, Val: 0.6940, Test: 0.7090\n",
      "Epoch: 194, Loss: 0.6835, Val: 0.6980, Test: 0.7150\n",
      "Epoch: 195, Loss: 0.7354, Val: 0.7000, Test: 0.7180\n",
      "Epoch: 196, Loss: 0.7988, Val: 0.7080, Test: 0.7240\n",
      "Epoch: 197, Loss: 0.6878, Val: 0.7120, Test: 0.7210\n",
      "Epoch: 198, Loss: 0.6732, Val: 0.7120, Test: 0.7200\n",
      "Epoch: 199, Loss: 0.6702, Val: 0.7160, Test: 0.7220\n",
      "Epoch: 200, Loss: 0.6963, Val: 0.7200, Test: 0.7240\n"
     ]
    }
   ],
   "source": [
    "# GAT\n",
    "\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, heads):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12)\n",
    "        self.conv1 = GATConv(dataset.num_features, hidden_channels) \n",
    "        self.conv2 = GATConv(hidden_channels, dataset.num_classes) \n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GAT(hidden_channels=8, heads=8)\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test(mask):\n",
    "      model.eval()\n",
    "      out = model(data.x, data.edge_index)\n",
    "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "      correct = pred[mask] == data.y[mask]  # Check against ground-truth labels.\n",
    "      acc = int(correct.sum()) / int(mask.sum())  # Derive ratio of correct predictions.\n",
    "      return acc\n",
    "\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    val_acc = test(data.val_mask)\n",
    "    test_acc = test(data.test_mask)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9643\n"
     ]
    }
   ],
   "source": [
    "test_acc = test(mask=data.train_mask)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2bac1bcfe5acfb15544f90e576792d202dc0c4fe4bcf21258a0f6f0edda6a391"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
